{"docstore/metadata": {"c1bacc19-f8c5-46d2-8700-edba3d028591": {"doc_hash": "6b127fbaa07c428fa3bc6160efa2a88780b35d64a5284977d4f1757a28b958cd"}, "c7f56648-03bf-43ff-bd36-67760b71d5fc": {"doc_hash": "82d2266e81d15c7cb031483c8fc81b9d7307392f73660abd00466326189dc0bc"}, "ae442b0d-e341-4d8b-8d2f-b74e9178de2c": {"doc_hash": "01fe585f227ab90502fdfd2a476100c9d47cfa49d7d2dc2f3887b7ce5b8dd5ca"}, "6a09b3ae-94e8-4b75-abad-fe72390fdb78": {"doc_hash": "a85aa3aa6b64d0226aa7d5d850587cd3e41e7be820c65969439583b2d608315b", "ref_doc_id": "c1bacc19-f8c5-46d2-8700-edba3d028591"}, "c57c7777-ae2c-47f6-81e0-719e7376a64d": {"doc_hash": "116fd0c6b21d17772beec2537a5fef4ee3ca266ce4a37eef6e0b01a9f47dfd53", "ref_doc_id": "c1bacc19-f8c5-46d2-8700-edba3d028591"}, "3808bb1e-2224-47fc-90be-10476691af6b": {"doc_hash": "1817f16bd24c78752f7ce5948a0a31a4db203118154eb1c814b737861dbe4470", "ref_doc_id": "c1bacc19-f8c5-46d2-8700-edba3d028591"}, "0baa119d-9d1c-49c5-b477-4b770837493c": {"doc_hash": "8893e0b4a08ad5dc87f89d43592b712357d62fcb98e3a9fb1428566cd62ccd1c", "ref_doc_id": "c1bacc19-f8c5-46d2-8700-edba3d028591"}, "c2ebac7f-4ff8-4d1a-94d6-640944f05a21": {"doc_hash": "ed80bee26fe94c83a5015fe23ed777ca3a7f7111831a332e38a16872b0b03e79", "ref_doc_id": "c7f56648-03bf-43ff-bd36-67760b71d5fc"}, "ecb380f4-5c8f-4287-84aa-d80b8f62ba29": {"doc_hash": "8e47c32b4209d72ae749e1a74cc5f816cd60d458e19399ea22baf5fe357657a2", "ref_doc_id": "c7f56648-03bf-43ff-bd36-67760b71d5fc"}, "6b2f0413-40c8-4376-8fa1-a77cd02b0413": {"doc_hash": "8c9cb6e40ddc418b24c02ad803c661298b10ab6ad4a151992d3b0368c9c5ec29", "ref_doc_id": "c7f56648-03bf-43ff-bd36-67760b71d5fc"}, "9fe50723-b7e8-454d-ac0d-85b0df17f3fe": {"doc_hash": "93392b37fa33ce6bf33053a5fc30354e9a3b8eafc48604fd6891e016bd7851a6", "ref_doc_id": "ae442b0d-e341-4d8b-8d2f-b74e9178de2c"}, "b31b112c-d9a8-45bb-8c53-cd003ce57449": {"doc_hash": "6a80c1c0d6eae97b73c0fe9302dc2e5df7ec10d590841e7f2ae5fa849ecdc245", "ref_doc_id": "ae442b0d-e341-4d8b-8d2f-b74e9178de2c"}, "c41fe86d-eee8-4139-b256-1eb3984fced3": {"doc_hash": "63e8281263967697f2e65c322d282d8fbc0ac17b24a8b468b20dfd53c2520e53", "ref_doc_id": "ae442b0d-e341-4d8b-8d2f-b74e9178de2c"}, "3c1a930c-4bf5-41b2-bdb4-4cb05db153da": {"doc_hash": "f7c7960c36fedaa1edaeeddf00b8c226567430acd077723535fd7a321a036142", "ref_doc_id": "ae442b0d-e341-4d8b-8d2f-b74e9178de2c"}}, "docstore/data": {"6a09b3ae-94e8-4b75-abad-fe72390fdb78": {"__data__": {"id_": "6a09b3ae-94e8-4b75-abad-fe72390fdb78", "embedding": null, "metadata": {"file_path": "data/public_new/18650l1", "file_name": "18650l1", "file_type": null, "file_size": 7637, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1bacc19-f8c5-46d2-8700-edba3d028591", "node_type": "4", "metadata": {"file_path": "data/public_new/18650l1", "file_name": "18650l1", "file_type": null, "file_size": 7637, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "hash": "6b127fbaa07c428fa3bc6160efa2a88780b35d64a5284977d4f1757a28b958cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c57c7777-ae2c-47f6-81e0-719e7376a64d", "node_type": "1", "metadata": {}, "hash": "116fd0c6b21d17772beec2537a5fef4ee3ca266ce4a37eef6e0b01a9f47dfd53", "class_name": "RelatedNodeInfo"}}, "hash": "a85aa3aa6b64d0226aa7d5d850587cd3e41e7be820c65969439583b2d608315b", "text": "\\documentclass[11pt]{article}\n\\usepackage{amsmath}\n\\usepackage{geometry}%[margin=1.2in]\n\\usepackage{amssymb}\n\\usepackage{amsthm}\n\\usepackage{epsfig}\n\\usepackage{setspace}\n\\usepackage{graphicx, xcolor, hyperref}\n\\setstretch{1.2}\n\\geometry{\n    textheight=9in,\n    textwidth=5.5in,\n    top=1in,\n    headheight=12pt,\n    headsep=25pt,\n    footskip=30pt\n}\n\\input{commands}\n\n\n\n\n\n\\begin{document}\n\n\\lecture{1 --- September 6, 2023}{Fall 2023}{Prof. Philippe Rigollet}{Anya Katsevich}\n\n\\section{Overview}\n\nStatistics is about \\textbf{analyzing} and drawing \\textbf{conclusions} from \\textbf{data}. \\\\\n\n\\noindent What methods do we use to \\textbf{analyze} data?\n\\begin{enumerate}\n\\item descriptive statistics: numbers that summarize the data (e.g. the mean) or visual representations (e.g. histograms)\n\\item estimation, confidence intervals, hypothesis testing, regression...\n\\item and many more! Most of which will be covered in this class. \n\\end{enumerate}\n\n\\noindent What kinds of \\textbf{conclusions} can we draw?\n\\begin{enumerate}\n\\item make predictions (how many goals will the US women's soccer team make at the World Cup?) \n\\item answer yes/no questions (can LLMs get an MIT degree? Does blue light make you age faster?)\n\\end{enumerate}\n\n\\noindent What \\emph{is} the \\textbf{data}? \\\\\nIndependent, identically distributed (i.i.d.) samples $X_1,X_2,\\dots, X_n\\sim P$, \\emph{where $P$ is unknown!} (See Section~\\ref{sec:kiss} for a concrete example).\\\\\n\n\\noindent The statistics pipeline: \n\\begin{equation}\n\\text{\\parbox{50pt}{i.i.d. data $X_i\\sim P$}}\\longrightarrow \\text{\\fbox{\\parbox{50pt}{statistical method}}} \\longrightarrow \n\\hat P\\approx P\n\\end{equation} \nThe fields of statistics and probability are opposites in the following sense:\n\\begin{enumerate}\n\\item\\textbf{Probability: given $P$, what can we say about data from $P$?} \\\\\n\\emph{Example: $P=\\mathcal N(0,1)$. Using probability, we can say a sample $X\\sim P$ lies in the interval $(-1,1)$ with probability  $0.682$.}\n\\item\\textbf{Statistics: given data from $P$, what can we say about $P$?} \\\\\n\\emph{Example: $X=100$. Using statistics, we can say $X$ is most likely \\emph{not} a sample from $\\mathcal N(0, 1)$, i.e. $P\\neq \\mathcal N(0,1)$.}\n\\end{enumerate}\n\n\\section{Key tools from probability}\nThough it is ``opposite\" of statistics, probability is the workhorse of statistical computations. So let's review some probability fundamentals. \n\n\\subsection{Mean and variance of i.i.d. averages} Let $X_1,\\dots, X_n$ be i.i.d. with $\\E[X_i]=\\mu$ and $\\V[X_i]=\\sigma^2$ (we will use the notation $\\V[X]$ for variance of $X$).", "start_char_idx": 0, "end_char_idx": 2585, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c57c7777-ae2c-47f6-81e0-719e7376a64d": {"__data__": {"id_": "c57c7777-ae2c-47f6-81e0-719e7376a64d", "embedding": null, "metadata": {"file_path": "data/public_new/18650l1", "file_name": "18650l1", "file_type": null, "file_size": 7637, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1bacc19-f8c5-46d2-8700-edba3d028591", "node_type": "4", "metadata": {"file_path": "data/public_new/18650l1", "file_name": "18650l1", "file_type": null, "file_size": 7637, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "hash": "6b127fbaa07c428fa3bc6160efa2a88780b35d64a5284977d4f1757a28b958cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a09b3ae-94e8-4b75-abad-fe72390fdb78", "node_type": "1", "metadata": {"file_path": "data/public_new/18650l1", "file_name": "18650l1", "file_type": null, "file_size": 7637, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "hash": "a85aa3aa6b64d0226aa7d5d850587cd3e41e7be820c65969439583b2d608315b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3808bb1e-2224-47fc-90be-10476691af6b", "node_type": "1", "metadata": {}, "hash": "1817f16bd24c78752f7ce5948a0a31a4db203118154eb1c814b737861dbe4470", "class_name": "RelatedNodeInfo"}}, "hash": "116fd0c6b21d17772beec2537a5fef4ee3ca266ce4a37eef6e0b01a9f47dfd53", "text": "\\\\\n\\emph{Example: $X=100$. Using statistics, we can say $X$ is most likely \\emph{not} a sample from $\\mathcal N(0, 1)$, i.e. $P\\neq \\mathcal N(0,1)$.}\n\\end{enumerate}\n\n\\section{Key tools from probability}\nThough it is ``opposite\" of statistics, probability is the workhorse of statistical computations. So let's review some probability fundamentals. \n\n\\subsection{Mean and variance of i.i.d. averages} Let $X_1,\\dots, X_n$ be i.i.d. with $\\E[X_i]=\\mu$ and $\\V[X_i]=\\sigma^2$ (we will use the notation $\\V[X]$ for variance of $X$). Then the \\emph{sample mean} is\n$$\\bar X_n:=\\frac1n\\sum_{i=1}^nX_i,$$ and \n$$\\E[\\bar X_n] = \\mu,\\quad\\V[\\bar X_n]=\\frac{\\sigma^2}{n}.$$ \nThis comes from the following calculations (make sure you understand each step):\n\\beqs\n\\E[\\bar X_n]&=\\E\\l[\\frac1n\\sum_{i=1}^nX_i\\r]=\\frac1n\\sum_{i=1}^n\\E[X_i] = \\mu,\\\\ \\\\\n\\V[\\bar X_n]&=\\V\\l[\\frac1n\\sum_{i=1}^nX_i\\r]=\\textcolor{blue}{\\frac{1}{n^2}\\V\\l[\\sum_{i=1}^nX_i\\r] = \\frac{1}{n^2}\\sum_{i=1}^n\\V[X_i]}=\\frac{n\\sigma^2}{n^2}=\\frac{\\sigma^2}{n}.\\eeqs\nThe equality in blue expresses the important property that for independent variables, the \\emph{sum of the variances is the variance of the sum.}\n\\subsection{The Law of Large Numbers (LLN)} Together, $\\E[\\bar X_n]=\\mu$ and $\\V[\\bar X_n]=\\sigma^2/n$ tell us that the fluctuations of $\\bar X_n$ around $\\mu$ get smaller and smaller as $n\\to\\infty$. This is expressed by the following law of large numbers (LLN):\n$$\\bar X_n\\to\\mu\\quad\\text{as}\\quad n\\to\\infty.$$\n\\subsection{The Central Limit Theorem (CLT)} We know that the fluctuations of $\\bar X_n$ around $\\mu$ are shrinking, but to do statistical inference, we need more fine-grained information about the distribution of $\\bar X_n$. This is where the \\emph{Central Limit Theorem} (CLT) comes in. \n\nTo motivate the CLT, note that \n$$\\E\\l[\\frac{\\sqrt n}{\\sigma}(\\bar X_n - \\mu)\\r]=0,\\quad \\V\\l[\\frac{\\sqrt n}{\\sigma}(\\bar X_n - \\mu)\\r]=1.$$ (Make sure you can do this calculation). It turns out that this scaled, centered random variable $(\\sqrt n/\\sigma)(\\bar X_n-\\mu)$ converges to our favorite distribution which also has mean 0 and variance 1: the normal distribution $\\mathcal N(0,1)$. \n\\begin{thm}{Central Limit Theorem}{clt}\nLet $X_i$, $i=1,\\dots,n$ be i.i.d. with mean $\\mu$ and variance $\\sigma^2$. Then\n$$\\frac{\\sqrt n}{\\sigma}(\\bar X_n - \\mu)\\rightsquigarrow\\mathcal N(0, 1),$$ where $\\rightsquigarrow$ denotes convergence in distribution.\n\\end{thm}\n\\noindent Convergence in distribution means that for all $a,b$ we have\n$$\\mathbb P\\l(a\\leq\\frac{\\sqrt n}{\\sigma}(\\bar X_n - \\mu)\\leq b\\r)\\to\\mathbb P(a\\leq Z\\leq b)\\quad\\text{as}\\quad n\\to\\infty,$$ where $Z\\sim\\mathcal N(0,1)$.\n\\begin{remark}\nNote that $(\\sqrt n/\\sigma)(\\bar X_n-\\mu)$ itself need not be normally distributed.", "start_char_idx": 2055, "end_char_idx": 4813, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3808bb1e-2224-47fc-90be-10476691af6b": {"__data__": {"id_": "3808bb1e-2224-47fc-90be-10476691af6b", "embedding": null, "metadata": {"file_path": "data/public_new/18650l1", "file_name": "18650l1", "file_type": null, "file_size": 7637, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1bacc19-f8c5-46d2-8700-edba3d028591", "node_type": "4", "metadata": {"file_path": "data/public_new/18650l1", "file_name": "18650l1", "file_type": null, "file_size": 7637, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "hash": "6b127fbaa07c428fa3bc6160efa2a88780b35d64a5284977d4f1757a28b958cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c57c7777-ae2c-47f6-81e0-719e7376a64d", "node_type": "1", "metadata": {"file_path": "data/public_new/18650l1", "file_name": "18650l1", "file_type": null, "file_size": 7637, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "hash": "116fd0c6b21d17772beec2537a5fef4ee3ca266ce4a37eef6e0b01a9f47dfd53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0baa119d-9d1c-49c5-b477-4b770837493c", "node_type": "1", "metadata": {}, "hash": "8893e0b4a08ad5dc87f89d43592b712357d62fcb98e3a9fb1428566cd62ccd1c", "class_name": "RelatedNodeInfo"}}, "hash": "1817f16bd24c78752f7ce5948a0a31a4db203118154eb1c814b737861dbe4470", "text": "with mean $\\mu$ and variance $\\sigma^2$. Then\n$$\\frac{\\sqrt n}{\\sigma}(\\bar X_n - \\mu)\\rightsquigarrow\\mathcal N(0, 1),$$ where $\\rightsquigarrow$ denotes convergence in distribution.\n\\end{thm}\n\\noindent Convergence in distribution means that for all $a,b$ we have\n$$\\mathbb P\\l(a\\leq\\frac{\\sqrt n}{\\sigma}(\\bar X_n - \\mu)\\leq b\\r)\\to\\mathbb P(a\\leq Z\\leq b)\\quad\\text{as}\\quad n\\to\\infty,$$ where $Z\\sim\\mathcal N(0,1)$.\n\\begin{remark}\nNote that $(\\sqrt n/\\sigma)(\\bar X_n-\\mu)$ itself need not be normally distributed. For example if $X_i$ is Bernoulli, then it takes value 0 or 1, so $\\bar X_n$ will take values in a discrete range: $\\{0, 1/n, 2/n,\\dots, (n-1)/n, 1\\}$, whereas the normal distribution has continuous range. \n\\end{remark}\n\\begin{remark}\nIf $(\\sqrt n/\\sigma)(\\bar X_n - \\mu)\\approx\\mathcal N(0, 1)$ then \\beq\\label{CLT2}\\bar X_n\\approx\\mathcal N\\l(\\mu,\\frac{\\sigma^2}{n}\\r).\\eeq This is the form in which we'll typically use the CLT. As a rule of thumb, the approximation is reasonably accurate for $n\\geq30$. \\end{remark}\n\\section{An example}\\label{sec:kiss}\n Do people prefer to turn their heads to the right when they kiss?\n \n In an experiment in \\emph{Nature}~\\cite{kiss}, $n=124$ couples were observed kissing. 80 of the couples turned their heads to the right when they kissed. That's a proportion of $80/124=0.645$, which is bigger than $0.5$. Can we conclude for sure that humans have a preference to turn to the right? In other words --- is $0.645$ really ``much bigger\" than 0.5? Statistics will help us make this quantitative.\n \nWe model the $n$ couples as $X_i\\iid\\Ber(p)$, $i=1,\\dots, n$, where ``Ber\" stands for Bernoulli. Specifically, we let\n$$X_i=\\begin{cases}1\\quad\\text{if turned right},\\\\\n0\\quad\\text{if turned left}.\n\\end{cases}$$\nNote that $\\bar X_n = \\frac1n\\sum_{i=1}^nX_i$ is precisely the proportion of couples who turned their heads to the right. We have observed $\\bar X_n=0.645$. \\\\\n\n\\noindent We now want to know: what is the probability of observing $\\bar X_n=0.645$ given that $p=1/2$? If this probability is sizeable, then it is reasonable that $p=1/2$ is the true value of $p$ which generated the data $X_i\\sim\\Ber(p)$, and we can't conclude that there is a tendency to kiss turning your head to the right. But if the probability is very small, then we can confidently conclude that a right-turning preference does exist. \\\\\n\nSo let's do this computation: first we need the mean and variance of the $X_i$. The mean of $\\Ber(p)$ is $p$ and the variance is $p(1-p)$, so if $p=1/2$ then $\\mu=\\E[X_i]=1/2$ and $\\sigma^2=\\V[X_i]=1/4$. By the CLT, we then have\n$$\\bar X_n\\approx\\mathcal N\\l(\\mu, \\frac{\\sigma^2}{n}\\r) = \\mathcal N\\l(1/2, \\frac{1/4}{124}\\r)=\\mathcal N(0.5, 0.002).$$\n\n\\begin{figure}[h]\n    \\center\n    \\includegraphics[scale=0.1]{Images/density.png}\n\\end{figure}\n\nWe get $\\mathbb P(\\bar X_n\\geq 0.645)\\approx\\mathbb P(\\mathcal N(0.5, 0.002)\\geq0.645)\\approx 0.003$.", "start_char_idx": 4293, "end_char_idx": 7220, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0baa119d-9d1c-49c5-b477-4b770837493c": {"__data__": {"id_": "0baa119d-9d1c-49c5-b477-4b770837493c", "embedding": null, "metadata": {"file_path": "data/public_new/18650l1", "file_name": "18650l1", "file_type": null, "file_size": 7637, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1bacc19-f8c5-46d2-8700-edba3d028591", "node_type": "4", "metadata": {"file_path": "data/public_new/18650l1", "file_name": "18650l1", "file_type": null, "file_size": 7637, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "hash": "6b127fbaa07c428fa3bc6160efa2a88780b35d64a5284977d4f1757a28b958cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3808bb1e-2224-47fc-90be-10476691af6b", "node_type": "1", "metadata": {"file_path": "data/public_new/18650l1", "file_name": "18650l1", "file_type": null, "file_size": 7637, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "hash": "1817f16bd24c78752f7ce5948a0a31a4db203118154eb1c814b737861dbe4470", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2ebac7f-4ff8-4d1a-94d6-640944f05a21", "node_type": "1", "metadata": {}, "hash": "ed80bee26fe94c83a5015fe23ed777ca3a7f7111831a332e38a16872b0b03e79", "class_name": "RelatedNodeInfo"}}, "hash": "8893e0b4a08ad5dc87f89d43592b712357d62fcb98e3a9fb1428566cd62ccd1c", "text": "By the CLT, we then have\n$$\\bar X_n\\approx\\mathcal N\\l(\\mu, \\frac{\\sigma^2}{n}\\r) = \\mathcal N\\l(1/2, \\frac{1/4}{124}\\r)=\\mathcal N(0.5, 0.002).$$\n\n\\begin{figure}[h]\n    \\center\n    \\includegraphics[scale=0.1]{Images/density.png}\n\\end{figure}\n\nWe get $\\mathbb P(\\bar X_n\\geq 0.645)\\approx\\mathbb P(\\mathcal N(0.5, 0.002)\\geq0.645)\\approx 0.003$. This is what's known as a p-value. Since it's tiny, we can be very confident that $1/2$ is \\emph{not} the right value, and that the true value of $p$ is bigger than $1/2$ (meaning, there \\emph{is} a preference to turn your head to the right).\n\n\\begin{thebibliography}{9}\n\\bibitem{kiss}\nOnur G\\\"{u}nt\\\"{u}rk\\\"{u}n. \\emph{Adult persistence of head-turning asymmetry}. Nature, 2003\n\\end{thebibliography}\n\n\\end{document}", "start_char_idx": 6875, "end_char_idx": 7637, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2ebac7f-4ff8-4d1a-94d6-640944f05a21": {"__data__": {"id_": "c2ebac7f-4ff8-4d1a-94d6-640944f05a21", "embedding": null, "metadata": {"file_path": "data/public_new/18650l2", "file_name": "18650l2", "file_type": null, "file_size": 6905, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c7f56648-03bf-43ff-bd36-67760b71d5fc", "node_type": "4", "metadata": {"file_path": "data/public_new/18650l2", "file_name": "18650l2", "file_type": null, "file_size": 6905, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "hash": "82d2266e81d15c7cb031483c8fc81b9d7307392f73660abd00466326189dc0bc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0baa119d-9d1c-49c5-b477-4b770837493c", "node_type": "1", "metadata": {"file_path": "data/public_new/18650l1", "file_name": "18650l1", "file_type": null, "file_size": 7637, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "hash": "8893e0b4a08ad5dc87f89d43592b712357d62fcb98e3a9fb1428566cd62ccd1c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecb380f4-5c8f-4287-84aa-d80b8f62ba29", "node_type": "1", "metadata": {}, "hash": "8e47c32b4209d72ae749e1a74cc5f816cd60d458e19399ea22baf5fe357657a2", "class_name": "RelatedNodeInfo"}}, "hash": "ed80bee26fe94c83a5015fe23ed777ca3a7f7111831a332e38a16872b0b03e79", "text": "\\documentclass[11pt]{article}\n\\usepackage{amsmath}\n\\usepackage{geometry}%[margin=1.2in]\n\\usepackage{amssymb}\n\\usepackage{amsthm,bbm,bm}\n\\usepackage{epsfig}\n\\usepackage{setspace}\n\\usepackage{graphicx, xcolor, hyperref, wrapfig, float}\n\\setstretch{1.2}\n\\geometry{\n    textheight=9in,\n    textwidth=5.5in,\n    top=1in,\n    headheight=12pt,\n    headsep=25pt,\n    footskip=30pt\n}\n\\input{commands.tex}\n\n\\begin{document}\n\n\\lecture{2 --- September 8, 2023}{Fall 2023}{Prof. Philippe Rigollet}{Anya Katsevich}\n\n\\section{Basic data visualization: the histogram} \nSuppose $x_1,\\dots,x_n$ are the GPAs of the students in this class (we have $n=130$). We can visualize the distribution of GPAs with a histogram. \\\\\n\n\\begin{figure}[h]\n    \\center\n    \\includegraphics[scale=0.1]{Images/histogram.png}\n\\end{figure}\n\n\\begin{itemize}\n\\item The area of the rectangle above $[a,b)$ is the proportion of GPAs between $a$ and $b$:\n\\beq\n\\text{area}= \\frac1n\\sum_{i=1}^n\\mathbbm{1}(a\\leq x_i < b),\n\\eeq where $\\mathbbm{1}(\\cdot)$ is called an \\emph{indicator} function. It evaluates to 1 if the statement inside the parentheses is true and it evaluates to 0 if the statement is false.\n\\item Since $\\text{area}=(b-a)\\times\\text{height}$, we get the height of the column by dividing the area by $b-a$. \n\\item \\textbf{Caution:} \\emph{only if} the bins are equally spaced can we visually judge the proportions of GPAs in each bin by looking at the heights. If the bins are \\emph{not} equally spaced then the heights don't tell us everything (a column could be unusually tall if it corresponds to a very small bin size.) \n\\end{itemize}\n\\subsection{Shapes}\nWe can also smooth out the histogram with a ``kernel density estimator\" (KDE), which we'll learn about in December. A smoothed out histogram tells us about the \\emph{shape} of the distribution; see Figure~\\ref{fig:shape}.\n\\begin{figure}[h]\n    \\center\n    \\includegraphics[scale=0.2]{Images/shapes.png}\n    \\caption{Shapes a distribution can take. Is there skew? Are there one or several modes?}\n    \\label{fig:shape}\n    \\end{figure}\n\\section{Summary statistics} We can summarize the data with a few summary statistics:\n\\begin{itemize}\n\\item \\textbf{mean}\n\\item \\textbf{standard deviation}\n\\item \\textbf{median}\n\\begin{itemize}\n\\item splits the data in half: \\\\\nhalf of the data points $x_1,\\dots, x_n$ are to the left and half to the right\n\\item Formally: $\\frac1n\\sum_{i=1}^n\\mathbbm{1}(x_i\\leq \\text{median})= 1/2$. \n\\end{itemize}\n\\item \\textbf{quantiles} \n\\begin{itemize}\n\\item a generalization of the median\n\\item \\textbf{the $q_\\alpha$ quantile} is a number such that $\\alpha$ of the data is \\emph{above} it. \n\\item Formally: $\\frac1n\\sum_{i=1}^n\\mathbbm{1}(x_i\\leq q_\\alpha)= 1-\\alpha$. \n\\item Important special cases: \\\\\n\\textbf{1st quartile} $Q_1=q_{0.75}$ (3/4 of the data is to the right),\\\\\n \\textbf{3rd quartile} $Q_3=q_{0.25}$ (1/4 of the data is to the right).\n\\item Note that $Q_1<\\text{median}<Q_3$. The median is the second quartile. \n\\end{itemize}\n\\item \\textbf{interquantile range IQR}$=Q_3-Q_1$.", "start_char_idx": 0, "end_char_idx": 3045, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ecb380f4-5c8f-4287-84aa-d80b8f62ba29": {"__data__": {"id_": "ecb380f4-5c8f-4287-84aa-d80b8f62ba29", "embedding": null, "metadata": {"file_path": "data/public_new/18650l2", "file_name": "18650l2", "file_type": null, "file_size": 6905, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c7f56648-03bf-43ff-bd36-67760b71d5fc", "node_type": "4", "metadata": {"file_path": "data/public_new/18650l2", "file_name": "18650l2", "file_type": null, "file_size": 6905, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "hash": "82d2266e81d15c7cb031483c8fc81b9d7307392f73660abd00466326189dc0bc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2ebac7f-4ff8-4d1a-94d6-640944f05a21", "node_type": "1", "metadata": {"file_path": "data/public_new/18650l2", "file_name": "18650l2", "file_type": null, "file_size": 6905, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "hash": "ed80bee26fe94c83a5015fe23ed777ca3a7f7111831a332e38a16872b0b03e79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b2f0413-40c8-4376-8fa1-a77cd02b0413", "node_type": "1", "metadata": {}, "hash": "8c9cb6e40ddc418b24c02ad803c661298b10ab6ad4a151992d3b0368c9c5ec29", "class_name": "RelatedNodeInfo"}}, "hash": "8e47c32b4209d72ae749e1a74cc5f816cd60d458e19399ea22baf5fe357657a2", "text": "\\item Formally: $\\frac1n\\sum_{i=1}^n\\mathbbm{1}(x_i\\leq q_\\alpha)= 1-\\alpha$. \n\\item Important special cases: \\\\\n\\textbf{1st quartile} $Q_1=q_{0.75}$ (3/4 of the data is to the right),\\\\\n \\textbf{3rd quartile} $Q_3=q_{0.25}$ (1/4 of the data is to the right).\n\\item Note that $Q_1<\\text{median}<Q_3$. The median is the second quartile. \n\\end{itemize}\n\\item \\textbf{interquantile range IQR}$=Q_3-Q_1$.\n\\end{itemize}\nFigure~\\ref{fig:summ} shows some of these statistics on a smoothed histogram.\n\n\\begin{figure}[h]\n    \\center\n    \\includegraphics[scale=0.15]{Images/Densitywithsummarystats.png}\n    \\caption{A smoothed histogram showing locations of the summary statistics. Note that the mean is to the right of the median because the distribution is right-skewed.}\n    \\label{fig:summ}\n    \\end{figure}\n    \n\\subsection{Robustness}\n\\emph{Outliers} are abnormally large or small values compared to the rest of the data. Formally:\\\\\n$$x_i \\text{ is an \\textbf{outlier} if}\\quad x_i>Q_3+1.5\\text{IQR}\\quad\\text{or}\\quad x_i < Q_1 - 1.5\\text{IQR}.$$\n\nThe mean and standard deviation are strongly affected by outliers. For example, very large outliers pull the mean to the right of the median, as in Figure~\\ref{fig:summ}. On the other hand, the median and IQR are robust to outliers (if the largest data point is doubled, say, this will not change the location of the median and IQR). The following table summarizes four important summary statistics.\n\\begin{center}\n\\begin{tabular}{ c|c } \n location& spread   \\\\  \n \\hline\nmean & std. dev.   \\\\ \n\\emph{median} & \\emph{IQR} % $\\leftarrow$ robust \\\\ \n\\end{tabular}\n\\end{center}\n\\vspace{-22pt}\\hspace{260pt}$\\leftarrow$ \\emph{robust }\n\\vspace{5pt}\n\n\\section{Visualizing summary statistics}\nAnother way to concisely depict summary statistics is with a \\emph{box plot}, also sometimes called a ``box and whiskers\" plot; see Figure~\\ref{fig:box}. The left and right endpoints of the box are $Q_1,Q_3$ respectively, and a line is drawn in between to denote the location of the median. The box is our visual representation for 75\\% of the data, and the location of the median between $Q_1$ and $Q_3$ conveys whether the distribution is skewed in one direction. \n\n\\begin{figure}[h]\n    \\center\n    \\includegraphics[scale=0.15]{Images/boxplot.png}\n    \\caption{A box plot}\n    \\label{fig:box}\n    \\end{figure}\n\nThe two line segments extending to the left and right from the box are the ``whiskers\". The length of the whiskers is $1.5$IQR, so that by definition, the outliers are to the left of the left whisker and to the right of the right whisker. The locations of the outliers are indicated explicitly on the boxplot. \n\n%`whiskers\" and ``box\". Conveys some sense of skew. Whiskers are drawn using the criterion for outliers. Outliers are drawn above and below the whiskers.\n\\subsection{Data with multiple variables: scatterplot and comparative boxplot} So far we've only considered one-dimensional data. But we could also have e.g. pairs $(X_i, Y_i)$. For example, $X_i$ denotes the number of days a month a student smokes marijuana, and $Y_i$ denotes the student's GPA. A common way to depict such data is with a scatterplot, as in Figure~\\ref{fig:scatter}. We simply plot the location of each data point in the $X$-$Y$ plane.", "start_char_idx": 2645, "end_char_idx": 5910, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b2f0413-40c8-4376-8fa1-a77cd02b0413": {"__data__": {"id_": "6b2f0413-40c8-4376-8fa1-a77cd02b0413", "embedding": null, "metadata": {"file_path": "data/public_new/18650l2", "file_name": "18650l2", "file_type": null, "file_size": 6905, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c7f56648-03bf-43ff-bd36-67760b71d5fc", "node_type": "4", "metadata": {"file_path": "data/public_new/18650l2", "file_name": "18650l2", "file_type": null, "file_size": 6905, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "hash": "82d2266e81d15c7cb031483c8fc81b9d7307392f73660abd00466326189dc0bc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ecb380f4-5c8f-4287-84aa-d80b8f62ba29", "node_type": "1", "metadata": {"file_path": "data/public_new/18650l2", "file_name": "18650l2", "file_type": null, "file_size": 6905, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "hash": "8e47c32b4209d72ae749e1a74cc5f816cd60d458e19399ea22baf5fe357657a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fe50723-b7e8-454d-ac0d-85b0df17f3fe", "node_type": "1", "metadata": {}, "hash": "93392b37fa33ce6bf33053a5fc30354e9a3b8eafc48604fd6891e016bd7851a6", "class_name": "RelatedNodeInfo"}}, "hash": "8c9cb6e40ddc418b24c02ad803c661298b10ab6ad4a151992d3b0368c9c5ec29", "text": "The locations of the outliers are indicated explicitly on the boxplot. \n\n%`whiskers\" and ``box\". Conveys some sense of skew. Whiskers are drawn using the criterion for outliers. Outliers are drawn above and below the whiskers.\n\\subsection{Data with multiple variables: scatterplot and comparative boxplot} So far we've only considered one-dimensional data. But we could also have e.g. pairs $(X_i, Y_i)$. For example, $X_i$ denotes the number of days a month a student smokes marijuana, and $Y_i$ denotes the student's GPA. A common way to depict such data is with a scatterplot, as in Figure~\\ref{fig:scatter}. We simply plot the location of each data point in the $X$-$Y$ plane.\n\\begin{figure}[H]\n    \\center\n    \\includegraphics[scale=0.13]{Images/scatterplot.png}\n    \\caption{A scatterplot}\n    \\label{fig:scatter}\n    \\end{figure}\n%  \\begin{wrapfigure}{r}{0.5\\textwidth}\\label{fig:scatter}\n%  \\vspace{-7pt}\n%  \\begin{center}\n%    \\includegraphics[width=0.48\\textwidth]{{\"18.650 Lecture Notes/scatterplot\"}.png}\n%  \\end{center}\n%  \\caption{A scatterplot}\n%\\end{wrapfigure}  \n\n\nIf the $X$-values tend to be clustered or if the $X$-values are not numbers at all (e.g. $X_i\\in\\{$freshman, sophomore, junior, senior$\\}$), then we can depict the data using several boxplots for the $Y$ distributions, one for each cluster/category of $X$ values. This lets us visualize the difference in the $Y$ distributions across different $X$ values. \nThis is called a comparative boxplot; see Figure~\\ref{fig:comp}. \n\\begin{figure}[h]\n    \\center\n    \\includegraphics[scale=0.1]{Images/compbox.png}\n    \\caption{Comparative boxplots}\n    \\label{fig:comp}\n    \\end{figure}\n\n\\end{document}", "start_char_idx": 5230, "end_char_idx": 6905, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9fe50723-b7e8-454d-ac0d-85b0df17f3fe": {"__data__": {"id_": "9fe50723-b7e8-454d-ac0d-85b0df17f3fe", "embedding": null, "metadata": {"file_path": "data/public_new/18650l3", "file_name": "18650l3", "file_type": null, "file_size": 7871, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae442b0d-e341-4d8b-8d2f-b74e9178de2c", "node_type": "4", "metadata": {"file_path": "data/public_new/18650l3", "file_name": "18650l3", "file_type": null, "file_size": 7871, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "hash": "01fe585f227ab90502fdfd2a476100c9d47cfa49d7d2dc2f3887b7ce5b8dd5ca", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b2f0413-40c8-4376-8fa1-a77cd02b0413", "node_type": "1", "metadata": {"file_path": "data/public_new/18650l2", "file_name": "18650l2", "file_type": null, "file_size": 6905, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "hash": "8c9cb6e40ddc418b24c02ad803c661298b10ab6ad4a151992d3b0368c9c5ec29", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b31b112c-d9a8-45bb-8c53-cd003ce57449", "node_type": "1", "metadata": {}, "hash": "6a80c1c0d6eae97b73c0fe9302dc2e5df7ec10d590841e7f2ae5fa849ecdc245", "class_name": "RelatedNodeInfo"}}, "hash": "93392b37fa33ce6bf33053a5fc30354e9a3b8eafc48604fd6891e016bd7851a6", "text": "\\documentclass[11pt]{article}\n\\usepackage{amsmath}\n\\usepackage{geometry}%[margin=1.2in]\n\\usepackage{amssymb}\n\\usepackage{amsthm,bbm,bm}\n\\usepackage{epsfig}\n\\usepackage{setspace}\n\\usepackage{graphicx, xcolor, hyperref, wrapfig, float}\n\\setstretch{1.2}\n\\geometry{\n    textheight=9in,\n    textwidth=5.5in,\n    top=1in,\n    headheight=12pt,\n    headsep=25pt,\n    footskip=30pt\n}\n\\input{commands.tex}\n\n\\begin{document}\n\n\\lecture{3 --- September 11, 2023}{Fall 2023}{Prof. Philippe Rigollet}{Anya Katsevich}\n\\section{Convergence of Random Variables} \nFor a sequence of numbers $x_n$, there is only one meaning of ``$x_n\\to x$ as $n\\to\\infty$\". But there are multiple ways that a sequence of random variables $X_n$ can converge to another random variable $X$. Here we go over two types of convergence.\n\\begin{defn}{Convergence in probability}{convprob}We say $X_n\\stackrel{\\mathbb P}{\\to}X$ as $n\\to\\infty$ (in words, ``$X_n$ converges to $X$ in probability\") if for every $\\epsilon>0$, it holds\n$$\\mathbb P(|X_n-X|>\\epsilon)\\to0\\quad\\text{as}\\quad n\\to\\infty.$$\n\\end{defn}\n\\begin{example} Suppose $X_n\\sim \\Ber(1/2)$ for all $n=1,2,\\dots$. Is it true that $X_n\\stackrel{\\mathbb P}{\\to} X\\sim\\Ber(1/2)$ for some other random variable $X$ that has the same distribution $\\Ber(1/2)$? Let's check this, supposing $X$ is independent of $X_n$. Note that $|X_n-X|$ is either 0 or 1. So if we take any $\\epsilon\\in(0,1)$, the event $\\{|X_n-X|>\\epsilon\\}$ is the same as the event $\\{|X_n-X|=1\\}$, and this occurs if $X_n=0$ and $X=1$ or if $X_n=1$ and $X=0$. Therefore,\n\\beqsn\n\\mathbb P(|X_n-X|>\\epsilon) &=\\mathbb P(\\{X_n=1\\cap X=0\\}\\cup\\{X_n=0\\cap X=1\\})\\\\\n&=\\mathbb P(X_n=1,X=0)+\\mathbb P(X_n=0,X=1)\\\\\n& = \\frac12\\times\\frac12+\\frac12\\times\\frac12=\\frac12.\\eeqsn This does \\emph{not} go to zero! So $X_n$ does not converge to $X$ in probability.\n\\end{example}\n\\begin{defn}{Convergence in distribution}{convdist}\nWe say $X_n\\rightsquigarrow X$ as $n\\to\\infty$ (in words, ``$X_n$ converges to $X$ in probability\") if \n$$\\mathbb P(X_n\\leq x)\\to\\mathbb P(X\\leq x)\\quad\\text{as}\\quad n\\to\\infty$$ for all $x$ at which the cdf $x\\mapsto\\mathbb P(X\\leq x)$ is continuous. \n\\end{defn}\n\\begin{example}Consider the same set-up as the previous example: $X_n\\sim\\Ber(1/2)$ for all $n$. Then indeed, $X_n\\rightsquigarrow\\Ber(1/2)$. Here we use the convention of indicating the limit $X$ by its distribution $\\Ber(1/2)$.\n\\end{example}\nWhat is the relationship between the two types of convergence? The next theorem shows that convergence in probability is stronger. \n\\begin{thm}{Relationship between convergence types}{thm:rel}\nIf $X_n\\stackrel{\\mathbb P}{\\to}X$ then $X_n\\rightsquigarrow X$. \n\\end{thm} Note the converse does not hold, as the above two Bernoulli examples demonstrate. \nCLT uses convergence in distribution.", "start_char_idx": 0, "end_char_idx": 2799, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b31b112c-d9a8-45bb-8c53-cd003ce57449": {"__data__": {"id_": "b31b112c-d9a8-45bb-8c53-cd003ce57449", "embedding": null, "metadata": {"file_path": "data/public_new/18650l3", "file_name": "18650l3", "file_type": null, "file_size": 7871, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae442b0d-e341-4d8b-8d2f-b74e9178de2c", "node_type": "4", "metadata": {"file_path": "data/public_new/18650l3", "file_name": "18650l3", "file_type": null, "file_size": 7871, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "hash": "01fe585f227ab90502fdfd2a476100c9d47cfa49d7d2dc2f3887b7ce5b8dd5ca", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9fe50723-b7e8-454d-ac0d-85b0df17f3fe", "node_type": "1", "metadata": {"file_path": "data/public_new/18650l3", "file_name": "18650l3", "file_type": null, "file_size": 7871, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "hash": "93392b37fa33ce6bf33053a5fc30354e9a3b8eafc48604fd6891e016bd7851a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c41fe86d-eee8-4139-b256-1eb3984fced3", "node_type": "1", "metadata": {}, "hash": "63e8281263967697f2e65c322d282d8fbc0ac17b24a8b468b20dfd53c2520e53", "class_name": "RelatedNodeInfo"}}, "hash": "6a80c1c0d6eae97b73c0fe9302dc2e5df7ec10d590841e7f2ae5fa849ecdc245", "text": "\\end{defn}\n\\begin{example}Consider the same set-up as the previous example: $X_n\\sim\\Ber(1/2)$ for all $n$. Then indeed, $X_n\\rightsquigarrow\\Ber(1/2)$. Here we use the convention of indicating the limit $X$ by its distribution $\\Ber(1/2)$.\n\\end{example}\nWhat is the relationship between the two types of convergence? The next theorem shows that convergence in probability is stronger. \n\\begin{thm}{Relationship between convergence types}{thm:rel}\nIf $X_n\\stackrel{\\mathbb P}{\\to}X$ then $X_n\\rightsquigarrow X$. \n\\end{thm} Note the converse does not hold, as the above two Bernoulli examples demonstrate. \nCLT uses convergence in distribution. LLN uses convergence in probability. \n\\begin{lma}{convergence to a constant}{relation}\nIf $X_n\\rightsquigarrow c$ for a deterministic constant $c$, then $X_n\\stackrel{\\mathbb P}{\\to}c$.\n\\end{lma}\n\\begin{proof}\n\\beqsn\n\\mathbb P(|X_n-c|>\\epsilon) &= \\mathbb P(X_n\\leq c-\\epsilon)+\\mathbb P(X_n\\geq c+\\epsilon)\\\\\n&\\to \\mathbb P(X\\leq c-\\epsilon) + \\mathbb P(X\\geq c+\\epsilon) = 0+0=0,\\eeqsn since $X=c$.\n\\end{proof}\n\\subsection{Operations which preserve convergence}\n\\begin{thm}{Convergence of sums and products}{op}\nIf $X_n\\stackrel{\\mathbb P}{\\to}X$ and $Y_n\\stackrel{\\mathbb P}{\\to}Y$ then $X_n+Y_n\\stackrel{\\mathbb P}{\\to}X+Y$ and $X_nY_n\\stackrel{\\mathbb P}{\\to}XY$. \\\\\n\nIf $X_n\\rightsquigarrow X$ and $Y_n\\stackrel{\\mathbb P}{\\to}c$ then $X_n+Y_n\\rightsquigarrow X+c$ and $X_nY_n\\rightsquigarrow Xc.$ \n\\end{thm} \nThe second statement is known as Slutsky's Theorem. \n\\begin{remark}In general, $X_n\\rightsquigarrow X$ and $Y_n\\rightsquigarrow Y$ does \\emph{not} imply $X_n+Y_n\\rightsquigarrow X+Y$. In fact, a statement like this does not even make sense, as the next example shows. \n\\end{remark}\n\\begin{example}\nSuppose $X_n\\sim\\mathcal N(0,1)$ for all $n$ so $X_n\\rightsquigarrow X$ \\emph{for any $X$ such that $X\\sim\\mathcal N(0,1)$}. Next, let $Y_n=-X_n$ for all $n$, so by symmetry of the standard normal, $Y_n\\sim\\mathcal N(0,1)$ as well. Therefore, $Y_n\\rightsquigarrow Y$ \\emph{for any $Y\\sim\\mathcal N(0,1)$}. \\\\\n\nSo does $0=X_n+Y_n$ converge in distribution to $X+Y$? This is true only if $Y=-X$! But it would be equally valid to choose $Y=X$, in which case $0$ does not converge to $X+Y=2X$. The problem is that we have no information about the correlation between the limits $X$ and $Y$, but we need this information to determine the distribution of $X+Y$.\n\\end{example}\n\n\\begin{thm}{Continuous Mapping Theorem}{cmt}\nIf $X_n\\stackrel{\\mathbb P}{\\to}X$ then $g(X_n)\\stackrel{\\mathbb P}{\\to}g(X)$ for continuous functions $g$. Similarly, if $X_n\\rightsquigarrow X$ then $g(X_n)\\rightsquigarrow g(X)$ for continuous $g$.\n\\end{thm}\n\\begin{thm}{Delta Method}{del}\nSuppose $\\sqrt n(Y_n-\\mu)/\\sigma\\rightsquigarrow Y\\sim \\mathcal N(0,1)$ for a sequence of random variables $Y_n$.", "start_char_idx": 2155, "end_char_idx": 4985, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c41fe86d-eee8-4139-b256-1eb3984fced3": {"__data__": {"id_": "c41fe86d-eee8-4139-b256-1eb3984fced3", "embedding": null, "metadata": {"file_path": "data/public_new/18650l3", "file_name": "18650l3", "file_type": null, "file_size": 7871, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae442b0d-e341-4d8b-8d2f-b74e9178de2c", "node_type": "4", "metadata": {"file_path": "data/public_new/18650l3", "file_name": "18650l3", "file_type": null, "file_size": 7871, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "hash": "01fe585f227ab90502fdfd2a476100c9d47cfa49d7d2dc2f3887b7ce5b8dd5ca", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b31b112c-d9a8-45bb-8c53-cd003ce57449", "node_type": "1", "metadata": {"file_path": "data/public_new/18650l3", "file_name": "18650l3", "file_type": null, "file_size": 7871, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "hash": "6a80c1c0d6eae97b73c0fe9302dc2e5df7ec10d590841e7f2ae5fa849ecdc245", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c1a930c-4bf5-41b2-bdb4-4cb05db153da", "node_type": "1", "metadata": {}, "hash": "f7c7960c36fedaa1edaeeddf00b8c226567430acd077723535fd7a321a036142", "class_name": "RelatedNodeInfo"}}, "hash": "63e8281263967697f2e65c322d282d8fbc0ac17b24a8b468b20dfd53c2520e53", "text": "The problem is that we have no information about the correlation between the limits $X$ and $Y$, but we need this information to determine the distribution of $X+Y$.\n\\end{example}\n\n\\begin{thm}{Continuous Mapping Theorem}{cmt}\nIf $X_n\\stackrel{\\mathbb P}{\\to}X$ then $g(X_n)\\stackrel{\\mathbb P}{\\to}g(X)$ for continuous functions $g$. Similarly, if $X_n\\rightsquigarrow X$ then $g(X_n)\\rightsquigarrow g(X)$ for continuous $g$.\n\\end{thm}\n\\begin{thm}{Delta Method}{del}\nSuppose $\\sqrt n(Y_n-\\mu)/\\sigma\\rightsquigarrow Y\\sim \\mathcal N(0,1)$ for a sequence of random variables $Y_n$. Then for any differentiable $g$ such that $g'(\\mu)\\neq0$, we have\n$$\\frac{\\sqrt n}{\\sigma}\\l(g(Y_n)-g(\\mu)\\r)\\rightsquigarrow\\mathcal N(0, g'(\\mu)^2).$$\n\\end{thm}\n\\begin{remark}\nThe theorem is typically applied for $Y_n=\\bar X_n$ (a sample average). \n\\end{remark}\n\\begin{proof}\nWe Taylor expand $g$ around the point $\\mu$: $g(Y_n)-g(\\mu)=g'(\\mu)(Y_n-\\mu)+\\dots$, where the dots represent negligible terms. We multiply both sides by $\\sqrt n/\\sigma$ to get\n$$\n\\frac{\\sqrt n}{\\sigma}(g(Y_n)-g(\\mu))\\approx g'(\\mu)\\l[\\frac{\\sqrt n}{\\sigma}(Y_n-\\mu)\\r]\\rightsquigarrow g'(\\mu)Y\n$$ using that the expression in square brackets converges to $Y\\sim\\mathcal N(0,1)$. But $g'(\\mu)Y$ has distribution $\\mathcal N(0,g'(\\mu)^2)$, and we are done.\n\\end{proof}\n\n\\section{Slutsky's theorem in statistics: an example} A humble Harvard grad claims that on average, Harvard grads make no more than 120K at graduation. Let's test this hypothesis. Suppose we collect the salaries $X_1,\\dots, X_n$ of $n=100$ recent grads, and we find that the sample mean is $\\bar X_n=121$K while the sample standard deviation is $\\hat\\sigma=0.3$K. \n\nAssume that our model for this data is that $X_1,\\dots, X_n$ are i.i.d. with mean $\\mu$ and variance $\\sigma^2$. We want to know: how likely is it to observe $\\bar X_n=121$K if $\\mu=120$K? \n\nBy the Central Limit Theorem, $\\bar X_n\\approx\\mathcal N(\\mu,\\sigma^2/n)$, and we've assumed $\\mu=120$K. However, we don't know the true value of $\\sigma$. We only have an estimate for it, namely the sample standard deviation $\\hat\\sigma$. It is tempting to just replace $\\sigma$ by $\\hat\\sigma$ in the CLT, and Slutsky's theorem allows us to do just this:\n\n\\beq\\label{approxclt}\\frac{\\sqrt n}{\\hat\\sigma}\\l(\\bar X_n - \\mu\\r) = \\l[\\frac{\\sqrt n}{\\sigma}(\\bar X_n - \\mu)\\r]\\times \\frac{\\sigma}{\\hat\\sigma} \\rightsquigarrow \\mathcal N(0,1),\\eeq because\n\\begin{itemize}\n\\item $\\frac{\\sqrt n}{\\sigma}(\\bar X_n - \\mu)\\rightsquigarrow\\mathcal N(0,1)$ by the CLT, and\n\\item $\\frac{\\sigma}{\\hat\\sigma}\\stackrel{\\mathbb P}{\\to} 1$ by the LLN, \n\\end{itemize}\nso Slutsky's Theorem (the second part of Theorem~\\ref{thm:op}) tells us the product of the two converges in distribution to $\\mathcal N(0,1)$.", "start_char_idx": 4404, "end_char_idx": 7182, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c1a930c-4bf5-41b2-bdb4-4cb05db153da": {"__data__": {"id_": "3c1a930c-4bf5-41b2-bdb4-4cb05db153da", "embedding": null, "metadata": {"file_path": "data/public_new/18650l3", "file_name": "18650l3", "file_type": null, "file_size": 7871, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae442b0d-e341-4d8b-8d2f-b74e9178de2c", "node_type": "4", "metadata": {"file_path": "data/public_new/18650l3", "file_name": "18650l3", "file_type": null, "file_size": 7871, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "hash": "01fe585f227ab90502fdfd2a476100c9d47cfa49d7d2dc2f3887b7ce5b8dd5ca", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c41fe86d-eee8-4139-b256-1eb3984fced3", "node_type": "1", "metadata": {"file_path": "data/public_new/18650l3", "file_name": "18650l3", "file_type": null, "file_size": 7871, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}, "hash": "63e8281263967697f2e65c322d282d8fbc0ac17b24a8b468b20dfd53c2520e53", "class_name": "RelatedNodeInfo"}}, "hash": "f7c7960c36fedaa1edaeeddf00b8c226567430acd077723535fd7a321a036142", "text": "From~\\eqref{approxclt} we conclude that\n$$\\bar X_n\\approx\\mathcal N(\\mu,\\hat\\sigma^2/n) = \\mathcal N(120, 0.3^2/100).$$\n\\begin{figure}[h]\n    \\center\n    \\vspace{-25pt}\n    \\includegraphics[scale=0.15]{Images/density3.png}\n    \\end{figure}\n\nWe see from the figure that $\\bar X_n=121$K is very unlikely under the distribution $\\mathcal N(120,0.03^2)$, so we conclude the Harvard grad's claim that the average income is $120$K was an underestimate. \n\nTo see why $\\sigma/\\hat\\sigma$ converges to 1 in probability, note that\n$$\\hat\\sigma^2:=\\frac1n\\sum_{i=1}^n(X_i-\\bar X_n)^2\\approx \\frac1n\\sum_{i=1}^n(X_i-\\mu)^2\\stackrel{\\mathbb P}{\\to}\\E[(X_1-\\mu)^2]=\\sigma^2$$ by the LLN.\n\\end{document}", "start_char_idx": 7183, "end_char_idx": 7871, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"c1bacc19-f8c5-46d2-8700-edba3d028591": {"node_ids": ["6a09b3ae-94e8-4b75-abad-fe72390fdb78", "c57c7777-ae2c-47f6-81e0-719e7376a64d", "3808bb1e-2224-47fc-90be-10476691af6b", "0baa119d-9d1c-49c5-b477-4b770837493c"], "metadata": {"file_path": "data/public_new/18650l1", "file_name": "18650l1", "file_type": null, "file_size": 7637, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}}, "c7f56648-03bf-43ff-bd36-67760b71d5fc": {"node_ids": ["c2ebac7f-4ff8-4d1a-94d6-640944f05a21", "ecb380f4-5c8f-4287-84aa-d80b8f62ba29", "6b2f0413-40c8-4376-8fa1-a77cd02b0413"], "metadata": {"file_path": "data/public_new/18650l2", "file_name": "18650l2", "file_type": null, "file_size": 6905, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}}, "ae442b0d-e341-4d8b-8d2f-b74e9178de2c": {"node_ids": ["9fe50723-b7e8-454d-ac0d-85b0df17f3fe", "b31b112c-d9a8-45bb-8c53-cd003ce57449", "c41fe86d-eee8-4139-b256-1eb3984fced3", "3c1a930c-4bf5-41b2-bdb4-4cb05db153da"], "metadata": {"file_path": "data/public_new/18650l3", "file_name": "18650l3", "file_type": null, "file_size": 7871, "creation_date": "2023-11-28", "last_modified_date": "2023-11-28", "last_accessed_date": "2023-11-28"}}}}